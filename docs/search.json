[
  {
    "objectID": "market.html",
    "href": "market.html",
    "title": "Job Market Hub",
    "section": "",
    "text": "This page is for internal use only. If you are not a letter writer, I encourage you to use the navigation tab to learn more about me and my work."
  },
  {
    "objectID": "market.html#welcome",
    "href": "market.html#welcome",
    "title": "Job Market Hub",
    "section": "",
    "text": "This page is for internal use only. If you are not a letter writer, I encourage you to use the navigation tab to learn more about me and my work."
  },
  {
    "objectID": "market.html#upcoming-deadlines",
    "href": "market.html#upcoming-deadlines",
    "title": "Job Market Hub",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\nLast update: August 23, 2024. Full list here.\nNumber of letters to write:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nNA - NA - NALetters:  Deadline: NA  Delivery: NA  Notes: NA  \n \n** - - **Letters:  Deadline:  Delivery:  Notes:"
  },
  {
    "objectID": "market.html#materials",
    "href": "market.html#materials",
    "title": "Job Market Hub",
    "section": "Materials",
    "text": "Materials\nMethods is the default set of materials\n\nCover letter [Methods] [Comparative] [Statistics]\nResearch statement [Methods] [Comparative] [Statistics]\nTeaching statement [Methods] [Comparative] [Statistics]\nDiversity statement [Methods] [Comparative] [Statistics]"
  },
  {
    "objectID": "market.html#research",
    "href": "market.html#research",
    "title": "Job Market Hub",
    "section": "Research",
    "text": "Research\nHighlights here. See the research tab for more.\n\nMethods\n\nAssessing the Validity of Prevalence Estimates in Double List Experiments\nJournal of Experimental Political Science\nPaper Appendix\n\nReview process complete, official version should come soon\nI started working on this before working on my dissertation!\n\nBalancing Precision and Retention in Experimental Design\nwith Erin Rossiter\nPaper Appendix\n\nUnder review in APSR\n\nList experiment and Network Scale-up questions to measure criminal governance strategies in Uruguay\nwith Ines Fynn, Verónica Pérez Bentancur, and Lucía Tiscornia\n\nOngoing project that should yield 4 papers:\n\n\nCombining list experiments and network scale-up questions to improve measurement of sensitive attitudes (Presenting at Polmeth, Maplemeth, Toronto Behavior Workshop, maybe LAPolmeth)\nRecovering estimates when placebo statements mess up your list experiment\nShort piece documenting extent of criminal governance activity in a context of low crime and high state presence (Presenting at APSA)\nLonger piece same as (3) but delving deeper into theory and field interviews\n\nSurvey Experiments and the Quest for Valid Interpretation\nwith Christopher Grady and James H. Kuklinski\nIn: Luigi Curini and Robert Franzese (eds)\nThe SAGE Handbook of Research Methods in Political Science and International Relations, 2020\nChapter Handbook\n\n\n\nComparative\n\nIgnoring Female Performance: A Survey Experiment on Policy Implementation in Argentina\nwith Virginia Oliveros, Rebecca Weitz-Shapiro, and Matthew S. Winters\nPaper Appendix\n\nUnder review at BJPS\n\nMayors Alter Spending to Counter the Electoral Consequences of Increased Monitoring: Evidence from Anti-Corruption Audits in Brazil\nPaper\n\nPresenting at APSA. Presented in political economy of development workshop organized by the Journal of Public Policy\n\nRevealing Nearby Corruption Drives Party Switching: Evidence from Local Level Audits in Brazil\nPaper\n\nOld job talk\n\nLight in the Midst of Chaos: COVID-19 and Female Political Representation\nwith Kelly Senters Piazza\nWorld Development 136: 105125, 2020\nPaper"
  },
  {
    "objectID": "market.html#teaching",
    "href": "market.html#teaching",
    "title": "Job Market Hub",
    "section": "Teaching",
    "text": "Teaching\n\nMcMaster course website: popw23.gustavodiaz.org\nSyllabi\nTeaching evaluations"
  },
  {
    "objectID": "market.html#other-stuff",
    "href": "market.html#other-stuff",
    "title": "Job Market Hub",
    "section": "Other stuff",
    "text": "Other stuff\nSection in progress. Things that may be missing or understated in current materials.\n\nWorking on text analysis modeling to analyze citation patterns in methodology across social sciences as part of postdoc duties\nAttending a grant writing workshop in the Fall, focus on preparing an application for an NSF-equivalent grant in Canada\nDissertation data collection combines text analysis and supervised learning to create most up to date data set of Brazilian local anti-corruption audits\nDissertation got poster award at Latin American Polmeth 2019\nCool places I have interviewed but not taken jobs at that may make people feel like they need to hire me before someone else does:\n\n\nPenn State, University of Konstanz, University of Birmingham, Howard University, William & Mary, KDI School of Public Policy and Management, Carlos III-Juan March Institute of Social Sciences, University of Melbourne\n\n\nWent to EITM at Emory in 2019\nHave experience mentoring an undergrad RA. Used it as an excuse so we could both learn text analysis. Student got job as data consultant after college. I claim all the credit\nLast 3 years have been marked by: Pandemic, parenting, moving, escaping from hurricane, immigration troubles, family illness. This is the first summer since 2020 without a major life thing going on. Feel free to weave that in your assessment of my productivity if you feel it may help"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Hub",
    "section": "",
    "text": "Last update: September 30, 2024. Full list here.\nNumber of letters to write:\n\n\n\n\n\nJake\nMatt\nMichelle\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n\n \nAssistant Professor in Political Methodology - Department of Political Science - University of Wisconsin-MadisonLetters: Jake, Matt, Michelle  Deadline: October 22, 2024  Delivery: Receive email  Notes: NA"
  },
  {
    "objectID": "index.html#upcoming-deadlines",
    "href": "index.html#upcoming-deadlines",
    "title": "Job Market Hub",
    "section": "",
    "text": "Last update: September 30, 2024. Full list here.\nNumber of letters to write:\n\n\n\n\n\nJake\nMatt\nMichelle\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n\n \nAssistant Professor in Political Methodology - Department of Political Science - University of Wisconsin-MadisonLetters: Jake, Matt, Michelle  Deadline: October 22, 2024  Delivery: Receive email  Notes: NA"
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Job Market Hub",
    "section": "Materials",
    "text": "Materials\n\nCV\nCover letter\nResearch statement\nTeaching statement\nDiversity statement"
  },
  {
    "objectID": "index.html#papers",
    "href": "index.html#papers",
    "title": "Job Market Hub",
    "section": "Papers",
    "text": "Papers\nTBD. See research and talks pages for now."
  },
  {
    "objectID": "index.html#teaching-portfolio",
    "href": "index.html#teaching-portfolio",
    "title": "Job Market Hub",
    "section": "Teaching portfolio",
    "text": "Teaching portfolio\nTBD. See teaching page for now."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "Job Market Hub",
    "section": "Highlights",
    "text": "Highlights"
  },
  {
    "objectID": "evals.html",
    "href": "evals.html",
    "title": "McMaster University",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\n\n \nThe following tables present a summary of my teaching evaluations. Unless otherwise noted, scores range from 1 to 5, with 5 being the most positive. Detailed evaluations available upon request."
  },
  {
    "objectID": "evals.html#teaching-evaluations",
    "href": "evals.html#teaching-evaluations",
    "title": "McMaster University",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\n\n \nThe following tables present a summary of my teaching evaluations. Unless otherwise noted, scores range from 1 to 5, with 5 being the most positive. Detailed evaluations available upon request."
  },
  {
    "objectID": "evals.html#selected-comments-from-students-at-tulane",
    "href": "evals.html#selected-comments-from-students-at-tulane",
    "title": "McMaster University",
    "section": "Selected comments from students at Tulane",
    "text": "Selected comments from students at Tulane\n\n“Professor Diaz is very understanding and does a great job applying concepts to class discussions and the out-of-class assignments reflect the key concepts of the course.”\n“I appreciated the flexible exam structure and edits made to the syllabus as a result of the challenges of this semester. I enjoyed the News Report assignments as they were an interesting way to interact with the course content in a meaningful way.”\n“The organization of content felt extremely coherent as we built on every previous module. This is no small feat given the subject matter. The mixture of quizzes and News Report assignments kept the course from being stale while still testing our knowledge of course material from differing vectors.”"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Job Market Hub",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\n\n\nAs someone teaching introductory courses in quantitative methods to social scientists, I face a polarized audience. Some students start their program with considerable experience on mathematical thinking, statistics, and statistical programming. Others start with an appreciation for quantitative research, but come from career paths designed to explicitly avoid math.\nMy approach to keep both audiences engaged within one term is to unify math, statistics, and coding as the task of acquiring a new language. A single course will not teach students everything they need to know to become fluent, but it can give enough tools to facilitate future learning in a direction that is beneficial to students with diverse backgrounds and career goals. For some, this may mean engaging directly with data and code or even creating new methodologies. For others, the goal may be just to communicate productively with scholarship drawing on quantitative findings or data analysts at the workplace.\nTo accommodate this diversity, I design courses with two principles in mind. First, students need flexibility to engage with the course on their own terms and focus on the content they find useful. For example, the flipped classroom lab sessions in my course on data analysis for public opinion and policy at McMaster asked students to evaluate a research design, suggest alternatives or modifications, and to evaluate its statistical properties through coding and writing. Some students may propose increasing the sample size, sampling from a different underlying population, or changing the assignment of treatment conditions. This allows students to pursue the tasks that suit their interests and gives me the freedom to reward creativity and effort over correctness. This principle also applies to the problem sets in my graduate introductory methods course, where contract grading allows me to reward learning even when student stuck on coding errors.\nThe second principle is accountability, which is necessary to keep everyone on task while allowing flexibility. This means agreeing on an overarching theme that every single course activity must relate to. For example, early on my data analysis for public opinion and policy course, I introduce the bias-variance tradeoff as a principle to choose among alternative research designs. So, while students are free to propose any modification to an existing research design that they deem appropriate, they are also required to document the explicit or implicit costs that would come from their proposal. They must consider, for instance, that a representative sample is more expensive than a convenience sample, or that implementing a block-randomized experiment may require access to variables that cannot be measured easily. I plan to apply the same principle in my upcoming seminar on methods for evidence-informed decision-making.\nFlexibility and accountability also help in preventing instances of discrimination in the learning process. Through flexibility, students are invited to add value to the course by bringing their own perspective, knowledge, or experiences. In turn, accountability sets the scope for the type of contributions of interventions that are admissible. From this perspective, a racist remark is unacceptable not because someone disagrees with it, but because it is beyond the scope of the course vocabulary."
  },
  {
    "objectID": "teaching.html#teaching-statement",
    "href": "teaching.html#teaching-statement",
    "title": "Job Market Hub",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\n\n\nAs someone teaching introductory courses in quantitative methods to social scientists, I face a polarized audience. Some students start their program with considerable experience on mathematical thinking, statistics, and statistical programming. Others start with an appreciation for quantitative research, but come from career paths designed to explicitly avoid math.\nMy approach to keep both audiences engaged within one term is to unify math, statistics, and coding as the task of acquiring a new language. A single course will not teach students everything they need to know to become fluent, but it can give enough tools to facilitate future learning in a direction that is beneficial to students with diverse backgrounds and career goals. For some, this may mean engaging directly with data and code or even creating new methodologies. For others, the goal may be just to communicate productively with scholarship drawing on quantitative findings or data analysts at the workplace.\nTo accommodate this diversity, I design courses with two principles in mind. First, students need flexibility to engage with the course on their own terms and focus on the content they find useful. For example, the flipped classroom lab sessions in my course on data analysis for public opinion and policy at McMaster asked students to evaluate a research design, suggest alternatives or modifications, and to evaluate its statistical properties through coding and writing. Some students may propose increasing the sample size, sampling from a different underlying population, or changing the assignment of treatment conditions. This allows students to pursue the tasks that suit their interests and gives me the freedom to reward creativity and effort over correctness. This principle also applies to the problem sets in my graduate introductory methods course, where contract grading allows me to reward learning even when student stuck on coding errors.\nThe second principle is accountability, which is necessary to keep everyone on task while allowing flexibility. This means agreeing on an overarching theme that every single course activity must relate to. For example, early on my data analysis for public opinion and policy course, I introduce the bias-variance tradeoff as a principle to choose among alternative research designs. So, while students are free to propose any modification to an existing research design that they deem appropriate, they are also required to document the explicit or implicit costs that would come from their proposal. They must consider, for instance, that a representative sample is more expensive than a convenience sample, or that implementing a block-randomized experiment may require access to variables that cannot be measured easily. I plan to apply the same principle in my upcoming seminar on methods for evidence-informed decision-making.\nFlexibility and accountability also help in preventing instances of discrimination in the learning process. Through flexibility, students are invited to add value to the course by bringing their own perspective, knowledge, or experiences. In turn, accountability sets the scope for the type of contributions of interventions that are admissible. From this perspective, a racist remark is unacceptable not because someone disagrees with it, but because it is beyond the scope of the course vocabulary."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Job Market Hub",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\nPolitical scientists often study phenomena that cannot be observed directly. For example, we use responses to hypothetical survey questions to infer actual behavior, we resort to aggregate election results to understand individual evaluations of politicians’ performance in office, and we conduct randomized controlled trials in some places to determine if a policy is advisable in other places. Doing this credibly requires careful research design, since researchers must try to anticipate the challenges to inference even before conducting data analysis.\nMy research develops standards to navigate the tradeoffs that emerge when one considers research design options before data collection. I use tools from causal inference and computational social science to identify practices and procedures that researchers can adopt to improve how they approach data collection at the pre-analysis stage. My current agenda focuses on improving statistical precision, since this is often the deciding factor when choosing among alternative research designs, all promising unbiased estimators.\nI apply the insights of my methodological work to questions in comparative politics and political behavior, especially around the challenges to accountability, governance, and representation. The common theme across these applications is the goal of improving our ability to make statistical inferences about hard-to-observe social and political phenomena.\nFor the sake of brevity, this document focuses on my methodological agenda only.\n\n\nThe last decade has seen considerable improvement in research transparency and registration. Recent advances in experimental design provide tools to diagnose the properties of a research design before data collection. For example, one can think about bias, power, or target sample size under different hypothetical data generation processes.\nA recurrent goal in statistics, econometrics, and political methodology is to minimize bias, or being close to the hypothetical truth on average. When planning an original data collection effort, researchers can often choose among many alternative research designs, all with previously identified unbiased estimators. My agenda focuses on optimizing statistical precision, understood as producing consistent results after multiple realizations of the same data generation process. While this is a crucial factor when choosing a research design, the literature implicitly assumes that one cam simply improve precision by increasing sample size, without providing much guidance to assess among alternative designs. Moreover, simply increasing sample size is not feasible in most applications due to practical or ethical considerations. Therefore, even when one does not face a choice between alternative designs, statistical precision is still paramount.\nFocusing on the design and analysis of experiments, my agenda advances the argument that the choice between alternative research designs for which unbiased estimators are already documented is rarely free. A design promising improvements in statistical precision without sacrificing unbiasedness often brings unforeseen costs in other dimensions.\nTwo projects exemplify how I develop standards to navigate research design choice under unforeseen challenges. First, In a manuscript with Erin Rossiter (Notre Dame), we discuss the circumstances under which adopting research design features aimed at improving precision can instead hurt it through implicit or explicit sample loss. For example, block randomization can improve precision, but if this requires contacting participants multiple times to collect pre-treatment blocking covariates, then it creates space for attrition that would not exist otherwise, which may offset the precision gains from blocking. We posit this is the main reason why researchers deviate from the standard experimental design infrequently. Through three replications and six reanalyses of previously published experiments in leading political science journals, we show how precision gains from alternative designs can withstand significant degrees of sample loss. From this exercise, we also identify guidelines to navigate the tradeoff between precision and sample retention in experiments.\nSecond, in a manuscript published at the Journal of Experimental Political Science, I propose statistical tests to address problems with double list experiments. Social scientists use list experiments in surveys when respondents may not answer truthfully to sensitive questions. When their assumptions are met, list experiments reduce sensitivity biases from misreporting. However, they tend to produce estimates with high variance, which prevents researchers from improving upon direct questioning. Double list experiments promise to remedy this by implementing two parallel list experiments and then aggregating their results, which roughly halves the variance of the estimate for the prevalence of the sensitive trait.\nThis implies an estimator that is more precise and still unbiased, but their implementation brings the question over whether the aggregation of the results of two parallel experiments yields a valid estimate. The tests leverage variation in the order in which respondents see the sensitive item to detect whether respondents are reacting to list experiment questions in unintended ways. This provides researchers with a tool to apply this underexplored variant of the technique more widely.\nThis agenda is currently extending toward improving precision by combining different techniques that target the same quantity of interest. For example, in work in progress with Inés Fynn (Universidad Católica del Uruguay), Verónica Pérez (Universidad de la República), and Lucía Tiscornia (University College Dublin), we combine list experiments and the network scale up method (NSUM), a popular technique in the health sciences, to improve precision in the estimation of the prevalence of sensitive attitudes and behaviors. Previous work combining different techniques to minimize sensitivity bias in survey questions relies on asking direct questions, altered research designs, cumbersome statistical modeling assumptions, or access to population-level data, all of which are problematic in their own way. By using NSUM questions as auxiliary information to the list experiment, we manage to improve precision by only imposing one additional assumption: that people with disproportionally high exposure to the sensitive trait of interest in their personal network are likely to hold the trait themselves. This does not apply to every sensitive attitude or behavior of interest in the social sciences, but is less demanding than the assumptions required to generalize NSUM estimates to a target population."
  },
  {
    "objectID": "research.html#research-statement",
    "href": "research.html#research-statement",
    "title": "Job Market Hub",
    "section": "",
    "text": "Gustavo Diaz\nDepartment of Political Science\nNorthwestern University\ngustavo.diaz@northwestern.edu\nPolitical scientists often study phenomena that cannot be observed directly. For example, we use responses to hypothetical survey questions to infer actual behavior, we resort to aggregate election results to understand individual evaluations of politicians’ performance in office, and we conduct randomized controlled trials in some places to determine if a policy is advisable in other places. Doing this credibly requires careful research design, since researchers must try to anticipate the challenges to inference even before conducting data analysis.\nMy research develops standards to navigate the tradeoffs that emerge when one considers research design options before data collection. I use tools from causal inference and computational social science to identify practices and procedures that researchers can adopt to improve how they approach data collection at the pre-analysis stage. My current agenda focuses on improving statistical precision, since this is often the deciding factor when choosing among alternative research designs, all promising unbiased estimators.\nI apply the insights of my methodological work to questions in comparative politics and political behavior, especially around the challenges to accountability, governance, and representation. The common theme across these applications is the goal of improving our ability to make statistical inferences about hard-to-observe social and political phenomena.\nFor the sake of brevity, this document focuses on my methodological agenda only.\n\n\nThe last decade has seen considerable improvement in research transparency and registration. Recent advances in experimental design provide tools to diagnose the properties of a research design before data collection. For example, one can think about bias, power, or target sample size under different hypothetical data generation processes.\nA recurrent goal in statistics, econometrics, and political methodology is to minimize bias, or being close to the hypothetical truth on average. When planning an original data collection effort, researchers can often choose among many alternative research designs, all with previously identified unbiased estimators. My agenda focuses on optimizing statistical precision, understood as producing consistent results after multiple realizations of the same data generation process. While this is a crucial factor when choosing a research design, the literature implicitly assumes that one cam simply improve precision by increasing sample size, without providing much guidance to assess among alternative designs. Moreover, simply increasing sample size is not feasible in most applications due to practical or ethical considerations. Therefore, even when one does not face a choice between alternative designs, statistical precision is still paramount.\nFocusing on the design and analysis of experiments, my agenda advances the argument that the choice between alternative research designs for which unbiased estimators are already documented is rarely free. A design promising improvements in statistical precision without sacrificing unbiasedness often brings unforeseen costs in other dimensions.\nTwo projects exemplify how I develop standards to navigate research design choice under unforeseen challenges. First, In a manuscript with Erin Rossiter (Notre Dame), we discuss the circumstances under which adopting research design features aimed at improving precision can instead hurt it through implicit or explicit sample loss. For example, block randomization can improve precision, but if this requires contacting participants multiple times to collect pre-treatment blocking covariates, then it creates space for attrition that would not exist otherwise, which may offset the precision gains from blocking. We posit this is the main reason why researchers deviate from the standard experimental design infrequently. Through three replications and six reanalyses of previously published experiments in leading political science journals, we show how precision gains from alternative designs can withstand significant degrees of sample loss. From this exercise, we also identify guidelines to navigate the tradeoff between precision and sample retention in experiments.\nSecond, in a manuscript published at the Journal of Experimental Political Science, I propose statistical tests to address problems with double list experiments. Social scientists use list experiments in surveys when respondents may not answer truthfully to sensitive questions. When their assumptions are met, list experiments reduce sensitivity biases from misreporting. However, they tend to produce estimates with high variance, which prevents researchers from improving upon direct questioning. Double list experiments promise to remedy this by implementing two parallel list experiments and then aggregating their results, which roughly halves the variance of the estimate for the prevalence of the sensitive trait.\nThis implies an estimator that is more precise and still unbiased, but their implementation brings the question over whether the aggregation of the results of two parallel experiments yields a valid estimate. The tests leverage variation in the order in which respondents see the sensitive item to detect whether respondents are reacting to list experiment questions in unintended ways. This provides researchers with a tool to apply this underexplored variant of the technique more widely.\nThis agenda is currently extending toward improving precision by combining different techniques that target the same quantity of interest. For example, in work in progress with Inés Fynn (Universidad Católica del Uruguay), Verónica Pérez (Universidad de la República), and Lucía Tiscornia (University College Dublin), we combine list experiments and the network scale up method (NSUM), a popular technique in the health sciences, to improve precision in the estimation of the prevalence of sensitive attitudes and behaviors. Previous work combining different techniques to minimize sensitivity bias in survey questions relies on asking direct questions, altered research designs, cumbersome statistical modeling assumptions, or access to population-level data, all of which are problematic in their own way. By using NSUM questions as auxiliary information to the list experiment, we manage to improve precision by only imposing one additional assumption: that people with disproportionally high exposure to the sensitive trait of interest in their personal network are likely to hold the trait themselves. This does not apply to every sensitive attitude or behavior of interest in the social sciences, but is less demanding than the assumptions required to generalize NSUM estimates to a target population."
  },
  {
    "objectID": "cover.html",
    "href": "cover.html",
    "title": "Job Market Hub",
    "section": "",
    "text": "Dear Members of the Search Committee,\nI write to express my interest in your call for an Assistant Professor in Political Methodology. I am an Assistant Professor of Instruction in the Department of Political Science at Northwestern University, where I teach courses on statistics, statistical programming, and computational social science and conduct research on quantitative methods and social science research design. I received my PhD in Political Science from the University of Illinois Urbana-Champaign in 2021. My work is published or forthcoming in outlets including the British Journal of Political Science, World Development, and the Journal of Experimental Political Science.\n\n\n\nMy research agenda focuses on using statistics to improve research design before data collection. My current focus is on statistical precision. This is overlooked in the statistics, econometrics, and political methodology literature in favor of identifying unbiased estimators. Implicitly, this literature assumes that one can improve statistical precision by just increasing sample size. This is not feasible in many social science applications due to practical or ethical considerations.\nFocusing on the design and analysis of surveys and experiments, this agenda seeks to shape applied research by focusing on cases where one can seemingly improve statistical precision without sacrificing unbiasedness. As I show in my work, this usually implies unforeseen costs in other dimensions.\nFor example, in “Balancing Precision and Retention in Experimental Design”, we discuss how implementing alternatives to the standard experimental design, such as block randomization, may attenuate expected precision gains via explicit or implicit sample loss, a concern that prevents researchers from applying these techniques widely. Through three replications and six reanalyses of previously published experiments in leading political science journals, we show how precision gains from alternative designs can withstand significant degrees of sample loss.\nAs another example, in a solo-authored publication in the Journal of Experimental Political Science, I discuss the unforeseen costs of implementing double list experiments. This is a variant of the list experiment that promises narrower confidence intervals but comes with under-explored questionnaire design complications in the form of carryover design effects, a special kind of question order effect. I introduce parametric and nonparametric statistical tests to diagnose this effect, which in turn facilitate the implementation of a more statistically efficient technique.\n\nOne of the core lessons from my research program on statistical precision is that combining different techniques helps overcome their respective limitations. For example, in work in progress, we combine list experiments with questions from the network scale up method (NSUM), a popular technique in the health sciences, to improve the estimation of sensitive attitudes and behaviors. On the one hand, list experiments suffer from low statistical precision. On the other hand, generalizing to a population of interest through NSUM requires assumptions that are untenable in social science applications. By using NSUM questions as auxiliary information to the list experiment, we improve precision without introducing cumbersome assumptions.\n\nMy research also influences substantive work in comparative politics and political behavior. In a working paper, we follow on our efforts to incorporate NSUM into social science applications by documenting the prevalence of criminal governance strategies in Uruguay. This is the basis of an European Research Council grant application seeking to understand criminal governance in least-expected contexts from a comparative perspective, which will in turn serve as a platform for further methodological innovation.\nAs another example, in a manuscript conditionally accepted at the British Journal of Political Science, we use a survey experiment in Argentina to study gendered differential reactions to policy implementation. Previous work suggests that women face higher scrutiny for their performance in office. However, in the context of the implementation of a food distribution program, we find that voters are only responsive to performance information among men officeholders and tend to ignore performance information when told that an officeholder is a woman. We attribute this result to voters’ perception of men politicians as the default category, thus providing no new information on top of performance. In turn, mentioning the gender of a woman politician leads voters to believe that good performance stems from factors beyond the incumbent’s control. This has broader implications for research on gender gaps in the evaluation of public officials, since most survey-experimental analysis on the topic fail to account for this informational imbalance.\n\n\nMy teaching focuses on making quantitative methods accessible to diverse audiences through a combination of flexibility and accountability. At Northwestern, I am the central person teaching quantitative methods in the department. I teach the first course in the PhD methods sequence, focusing on probability and statistical inference. I also lead the math camp for incoming political science and sociology students and run the year-long R workshop that introduces cutting-edge statistical programming practices. Later this year, I will teach the undergrad-level introduction to empirical research in political science, and a seminar on causal inference and machine learning methods for evidence-informed decision-making.\nBefore joining Northwestern, I taught data analysis for public policy and public opinion at McMaster and evidence-based policy to address social and political challenges in developing democracies at Tulane. Both courses emphasized experimental and quasi-experimental designs to generate credible evidence. Beyond the classroom, my previous role as the methods editorial assistant for the American Political Science Review gave me the opportunity to shape and influence the development and application of cutting-edge methods in the field, a goal that I continue to pursue through service and mentoring.\n\n\n\n\n\nI am prepared to teach courses on statistics, research design, experiments, causal inference, machine learning, and computational social science. You can find copies of current and future syllabi in my website.\n\nI believe my expertise makes me an excellent fit at Wisconsin. If you have any questions, you can contact me via email or phone.\nSincerely,"
  }
]